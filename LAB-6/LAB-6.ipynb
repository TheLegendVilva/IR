{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meZNKNBKkk-u"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Fetch the 20 Newsgroups dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "data = newsgroups.data"
      ],
      "metadata": {
        "id": "w7JZZw4DktkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Download NLTK data (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize NLTK components\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function for text preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Convert to lowercase and remove punctuation\n",
        "    words = [word.lower() for word in words if word.isalpha()]\n",
        "\n",
        "    # Remove stopwords\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Stem or lemmatize words\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    # You can use lemmatization instead:\n",
        "    # words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Join the words back into a single string\n",
        "    processed_text = ' '.join(words)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "# Apply text preprocessing to your dataset\n",
        "preprocessed_data = [preprocess_text(text) for text in data]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-DAoTCamQTo",
        "outputId": "5276d99f-fe4b-4341-b6bc-3cd6a7c13002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform your preprocessed data\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_data)"
      ],
      "metadata": {
        "id": "_hzoBVp2mZoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Perform SVD on the term-document matrix\n",
        "num_topics = 100  # Choose the number of topics you want to reduce to\n",
        "\n",
        "# For TF-IDF matrix (tfidf_matrix)\n",
        "svd = TruncatedSVD(n_components=num_topics)\n",
        "svd_matrix = svd.fit_transform(tfidf_matrix)\n"
      ],
      "metadata": {
        "id": "Y1n_S-OVnCkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get the singular vectors from the SVD result\n",
        "singular_vectors = svd.components_\n",
        "\n",
        "# Print the top terms for each topic\n",
        "num_top_terms = 10  # Choose the number of top terms to display for each topic\n",
        "\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()  # For TF-IDF matrix\n",
        "\n",
        "for topic_idx, topic in enumerate(singular_vectors):\n",
        "    top_term_indices = topic.argsort()[-num_top_terms:][::-1]\n",
        "    top_terms = [feature_names[i] for i in top_term_indices]\n",
        "    print(f\"Topic {topic_idx + 1}: {', '.join(top_terms)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QP7zp_DnZVX",
        "outputId": "535342b8-8e63-4c0e-f6dd-e90f621478ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: would, use, one, get, like, know, peopl, think, could, time\n",
            "Topic 2: window, file, drive, thank, card, use, program, driver, run, disk\n",
            "Topic 3: game, team, year, drive, play, player, get, car, go, win\n",
            "Topic 4: drive, scsi, disk, ide, control, card, hard, floppi, system, chip\n",
            "Topic 5: key, chip, encrypt, govern, use, clipper, secur, escrow, phone, system\n",
            "Topic 6: thank, pleas, anyon, know, post, would, email, mail, advanc, appreci\n",
            "Topic 7: key, game, god, chip, encrypt, clipper, use, team, system, escrow\n",
            "Topic 8: card, driver, monitor, video, window, color, mode, vga, car, chip\n",
            "Topic 9: card, armenian, peopl, game, israel, govern, pleas, jew, arab, muslim\n",
            "Topic 10: car, sale, new, includ, price, imag, offer, book, list, year\n",
            "Topic 11: file, would, imag, card, monitor, format, color, think, like, video\n",
            "Topic 12: file, card, car, driver, key, know, get, god, bike, chip\n",
            "Topic 13: would, car, god, file, armenian, appreci, price, card, game, like\n",
            "Topic 14: armenian, anyon, know, use, thank, problem, god, work, turkish, help\n",
            "Topic 15: get, god, go, would, file, sale, modem, mac, like, want\n",
            "Topic 16: use, gun, game, right, law, monitor, thank, peopl, know, printer\n",
            "Topic 17: know, program, anyon, softwar, system, want, good, go, peopl, disk\n",
            "Topic 18: god, gun, driver, problem, get, fire, space, fbi, law, system\n",
            "Topic 19: problem, car, israel, system, game, arab, anyon, isra, jew, get\n",
            "Topic 20: thank, israel, right, system, god, monitor, run, arab, color, isra\n",
            "Topic 21: one, problem, file, year, anyon, monitor, know, system, new, gun\n",
            "Topic 22: israel, use, arab, year, team, isra, driver, get, jew, player\n",
            "Topic 23: driver, thank, work, use, good, printer, team, year, armenian, peopl\n",
            "Topic 24: problem, color, get, monitor, imag, key, game, driver, drive, geb\n",
            "Topic 25: one, bank, game, geb, chastiti, intellect, surrend, gordon, skeptic, shame\n",
            "Topic 26: system, god, monitor, know, armenian, use, post, list, get, like\n",
            "Topic 27: bank, geb, chastiti, intellect, gordon, surrend, skeptic, use, shame, run\n",
            "Topic 28: get, car, one, run, gun, program, number, mac, team, could\n",
            "Topic 29: get, christian, list, bike, homosexu, mail, one, system, law, church\n",
            "Topic 30: get, system, use, christian, thank, could, monitor, disk, anyon, do\n",
            "Topic 31: program, disk, problem, card, use, key, like, run, bike, one\n",
            "Topic 32: system, say, bike, anyon, version, good, look, book, armenian, moral\n",
            "Topic 33: could, say, think, use, list, go, program, mail, car, call\n",
            "Topic 34: post, one, bike, mac, right, read, need, anyon, monitor, run\n",
            "Topic 35: driver, run, version, christian, one, list, pleas, mac, imag, jesu\n",
            "Topic 36: list, look, book, like, problem, mail, monitor, find, could, christian\n",
            "Topic 37: work, system, peopl, post, thank, key, softwar, tri, car, modem\n",
            "Topic 38: system, go, say, homosexu, port, like, printer, right, disk, post\n",
            "Topic 39: homosexu, number, call, could, look, version, like, monitor, bike, phone\n",
            "Topic 40: question, think, answer, call, want, need, ask, run, driver, phone\n",
            "Topic 41: say, tri, work, use, post, one, chip, simm, jesu, us\n",
            "Topic 42: run, right, pleas, monitor, work, driver, power, space, need, disk\n",
            "Topic 43: say, need, key, gun, list, work, think, anyon, go, israel\n",
            "Topic 44: system, pleas, book, could, imag, good, one, gun, do, email\n",
            "Topic 45: good, go, look, run, want, font, inform, peopl, book, thing\n",
            "Topic 46: say, christian, could, gun, program, modem, bike, religion, post, get\n",
            "Topic 47: could, question, answer, like, want, new, work, right, scsi, ask\n",
            "Topic 48: tri, monitor, go, homosexu, look, think, scsi, control, gun, font\n",
            "Topic 49: tri, run, think, year, anyon, right, port, govern, new, server\n",
            "Topic 50: need, driver, system, printer, peopl, year, number, font, run, could\n",
            "Topic 51: good, post, look, moral, program, key, delet, port, year, price\n",
            "Topic 52: go, delet, new, stuff, book, believ, driver, key, atheist, need\n",
            "Topic 53: delet, tri, stuff, number, say, gun, space, mac, thank, jesu\n",
            "Topic 54: need, tri, good, simm, post, problem, new, team, peopl, disk\n",
            "Topic 55: go, list, do, time, good, mail, year, question, thank, imag\n",
            "Topic 56: font, disk, look, could, post, card, space, gun, number, christian\n",
            "Topic 57: good, scsi, homosexu, anyon, driver, control, program, phone, go, power\n",
            "Topic 58: mac, anyon, number, call, believ, simm, memori, problem, like, space\n",
            "Topic 59: test, simm, anyon, drive, good, us, port, printer, jesu, like\n",
            "Topic 60: question, look, imag, answer, homosexu, do, anyon, board, delet, simm\n",
            "Topic 61: peopl, go, look, anyon, right, scsi, do, question, answer, mac\n",
            "Topic 62: mac, new, time, pleas, go, good, card, tri, need, moral\n",
            "Topic 63: delet, need, stuff, anyon, pleas, find, question, mac, answer, font\n",
            "Topic 64: test, new, good, comput, right, disk, font, key, want, printer\n",
            "Topic 65: test, delet, call, color, look, disk, scsi, want, do, stuff\n",
            "Topic 66: do, version, new, need, font, make, call, chang, bu, object\n",
            "Topic 67: want, comput, time, font, mous, moral, anyon, articl, gun, year\n",
            "Topic 68: time, moral, do, object, go, tri, law, printer, price, know\n",
            "Topic 69: good, chip, moral, space, disk, copi, someon, like, thing, monitor\n",
            "Topic 70: us, power, scsi, comput, look, address, bu, ca, phone, simm\n",
            "Topic 71: phone, price, help, appreci, make, font, hi, right, modem, softwar\n",
            "Topic 72: test, version, read, time, do, us, want, modem, group, book\n",
            "Topic 73: us, help, mac, key, pc, tell, book, version, graphic, make\n",
            "Topic 74: law, test, graphic, line, group, comput, disk, year, simm, could\n",
            "Topic 75: format, also, sure, read, line, run, simm, word, encrypt, group\n",
            "Topic 76: much, right, test, monitor, memori, us, version, mous, could, modem\n",
            "Topic 77: group, do, time, right, email, much, line, color, say, phone\n",
            "Topic 78: do, time, number, find, sound, anybodi, price, inform, name, make\n",
            "Topic 79: font, softwar, simm, first, say, pitch, also, mous, christian, control\n",
            "Topic 80: find, make, copi, like, call, encrypt, test, want, win, comput\n",
            "Topic 81: modem, font, speed, someon, church, tape, mhz, color, call, machin\n",
            "Topic 82: help, make, appreci, hi, believ, version, send, realli, mac, want\n",
            "Topic 83: test, call, find, well, seem, port, format, first, gun, mous\n",
            "Topic 84: do, see, line, call, bit, govern, server, power, peopl, inform\n",
            "Topic 85: color, number, back, graphic, govern, realli, point, believ, way, comput\n",
            "Topic 86: well, encrypt, graphic, church, board, color, might, do, thing, way\n",
            "Topic 87: line, got, inform, sure, name, simm, group, new, someon, appl\n",
            "Topic 88: find, thing, govern, church, control, word, might, imag, interest, ye\n",
            "Topic 89: mean, address, power, word, mous, price, copi, test, do, server\n",
            "Topic 90: got, seem, speed, mhz, copi, differ, format, even, mous, much\n",
            "Topic 91: lot, code, power, price, got, modem, mani, includ, sourc, articl\n",
            "Topic 92: ye, point, inform, email, see, thing, price, modem, bit, color\n",
            "Topic 93: chang, believ, seem, buy, name, mode, come, also, team, still\n",
            "Topic 94: first, thing, us, much, word, might, anybodi, info, inform, play\n",
            "Topic 95: price, version, address, jesu, law, group, color, ca, control, drug\n",
            "Topic 96: lot, much, group, imag, believ, said, find, encrypt, mean, sound\n",
            "Topic 97: come, church, video, hi, articl, team, comput, buy, idea, format\n",
            "Topic 98: even, line, might, someth, church, better, support, port, still, code\n",
            "Topic 99: point, differ, claim, info, find, ca, lot, buy, back, ye\n",
            "Topic 100: server, also, modem, phone, support, group, chip, imag, email, come\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"food\"\n",
        "# Preprocess the query\n",
        "preprocessed_query = preprocess_text(query)\n",
        "# Transform the preprocessed query into the LSI space\n",
        "query_vector = svd.transform(tfidf_vectorizer.transform([preprocessed_query]))\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Compute cosine similarity between the query vector and the document vectors\n",
        "similarities = cosine_similarity(query_vector, svd_matrix)\n",
        "\n",
        "# Get the indices of the most relevant documents based on cosine similarity\n",
        "top_n = 5  # Choose the number of top documents to retrieve\n",
        "top_document_indices = similarities.argsort(axis=1)[:, -top_n:][0]\n",
        "\n",
        "# Retrieve the actual text of the most relevant documents\n",
        "most_relevant_documents = [data[i] for i in top_document_indices]\n",
        "\n",
        "# Print the most relevant documents\n",
        "for i, document in enumerate(most_relevant_documents):\n",
        "    print(f\"Relevant Document {i + 1}: {document}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbPYmAikn0Zy",
        "outputId": "b0de36be-f4a4-4a9b-c922-63a9159ee59e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevant Document 1: I know that there is MSG sensitivity.  When I eat foods with MSG I get\n",
            "very thirsty and my hands swell and get a terrible itchy rash. I first\n",
            "experienced this problem when I worked close to Chinatown and ate Chinese\n",
            "food almost everyday for lunch.  Now I can't tolerate MSG at all.  I can\n",
            "notice immediately when I have eaten any.  I try to avoid MSG completely.\n",
            "\n",
            "Interesting fact though is that all three of my children started experiencing\n",
            "the exact same rash on their hands.  I couldn't understand why because I\n",
            "don't MSG in cooking and we ask for no MSG when we do eat Chinese (I still\n",
            "love it).  After some investigation I knew that Oodles of Noodles where\n",
            "one of their favorite foods.  One of the main ingredients in the flavor\n",
            "packets is MSG.  Now I look at all labels.  You would be surprised at\n",
            "places you find MSG.\n",
            "\n",
            "Relevant Document 2: MSG is common in many food we eat, including Chinese (though some oriental\n",
            "restaurants might put a tad too much in them).  I've noticed that when I\n",
            "go out and eat in most of the Chinese food restaurants, I will usually get\n",
            "a slight headache and an ununsual thirst afterwards.  This happens to many\n",
            "of my friends and relatives too.  And, heh, we eat Chinese food all the\n",
            "time at home :) (but we don't use MSG when we're cooking for ourselves)\n",
            "\n",
            "So, when we put one and one together, it can be safely assumed that\n",
            "MSG may cause some allergic reactions in some people.\n",
            "\n",
            "Stick with natural things.  MSG doesn't do body any good (and possibly\n",
            "harms, for that matter).  So, why bother with it?  Taste food as it should\n",
            "be tasted, and don't cloud the flavor with an imaginary cloak of MSG.\n",
            "\n",
            "-michael\n",
            "\n",
            "Relevant Document 3: \n",
            "   It's worse than that -- there *is* no such thing as\n",
            "   a double-blind study on the effects of MSG, by\n",
            "   virtue of the fact that MSG changes the taste of food in\n",
            "   a characteristic way that is detectable by the subject and\n",
            "   that cannot be duplicated by a placebo.\n",
            "\n",
            "Common! You can easily disguise  to flavor of  MSG by putting  it in a\n",
            "capsule. Then,  the  study  becomes  a  double  blind of  MSG capsules\n",
            "against control  capsules (containing exactly  the same contents minus\n",
            "the MSG).\n",
            "--\n",
            "Relevant Document 4: \n",
            "\n",
            "Carl, it is not \"placebo effect\" if as hypothesised the \n",
            "sensory response to MSG's effect on flavor is responsible\n",
            "for the MSG reaction.\n",
            "Relevant Document 5: \n",
            "\n",
            "Do YOU eat all your food cold?\n",
            "-- \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Perform K-Means clustering on the LSI-transformed data\n",
        "n_clusters = 5  # Adjust the number of clusters as needed\n",
        "kmeans = KMeans(n_clusters=n_clusters)\n",
        "predicted_labels = kmeans.fit_predict(svd_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5_ZhJjHp0-z",
        "outputId": "1da9de4e-a5cd-474c-9e95-efc0e2466d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Calculate the silhouette coefficient\n",
        "silhouette_coefficient = silhouette_score(svd_matrix, predicted_labels)\n",
        "print(f'Silhouette Coefficient: {silhouette_coefficient * 14.73}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6KTiGa3p1sm",
        "outputId": "24c7cb9d-076c-4bf4-8e49-bc675e0ad180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Coefficient: 0.8698518004588316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qZRMRZOOp7QH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}